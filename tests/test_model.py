#!/usr/bin/env python3
"""
æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ - æ‹¡å¼µç‰ˆå¯¾å¿œ
ã‚ãªãŸã®ãƒ¡ãƒ¼ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ã4ã‚«ãƒ†ã‚´ãƒªåˆ†é¡å¯¾å¿œ
"""

import pytest
import pandas as pd
import sys
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.train_model import create_extended_training_data, train_extended_model

def test_extended_sample_data_creation():
    """æ‹¡å¼µç‰ˆã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆãƒ†ã‚¹ãƒˆ"""
    df = create_extended_training_data()
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯
    assert isinstance(df, pd.DataFrame)
    assert len(df) == 45  # æ‹¡å¼µç‰ˆã¯45ä»¶
    assert 'subject' in df.columns
    assert 'body' in df.columns
    assert 'label' in df.columns
    
    # æ‹¡å¼µç‰ˆåˆ†é¡ãƒ©ãƒ™ãƒ«ã®ç¢ºèª
    expected_labels = ['æ”¯æ‰•ã„é–¢ä¿‚', 'ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³', 'é‡è¦', 'ä»•äº‹ãƒ»å­¦ç¿’']
    actual_labels = df['label'].unique()
    assert all(label in actual_labels for label in expected_labels)
    
    # å„ã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿æ•°ç¢ºèª
    label_counts = df['label'].value_counts()
    assert label_counts['æ”¯æ‰•ã„é–¢ä¿‚'] >= 10  # æ”¯æ‰•ã„é–¢ä¿‚ãŒååˆ†ãªæ•°
    assert label_counts['ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³'] >= 8  # ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ãŒååˆ†ãªæ•°
    assert label_counts['é‡è¦'] >= 7  # é‡è¦ãŒååˆ†ãªæ•°
    assert label_counts['ä»•äº‹ãƒ»å­¦ç¿’'] >= 5  # ä»•äº‹ãƒ»å­¦ç¿’ãŒååˆ†ãªæ•°

def test_extended_model_training():
    """æ‹¡å¼µç‰ˆãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ†ã‚¹ãƒˆ"""
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
    df = create_extended_training_data()
    vectorizer, model = train_extended_model()
    
    # ãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯
    assert vectorizer is not None
    assert model is not None
    
    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒ4ã‚«ãƒ†ã‚´ãƒªå¯¾å¿œã—ã¦ã„ã‚‹ã‹ç¢ºèª
    assert len(model.classes_) == 4
    expected_classes = ['æ”¯æ‰•ã„é–¢ä¿‚', 'ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³', 'é‡è¦', 'ä»•äº‹ãƒ»å­¦ç¿’']
    assert all(cls in model.classes_ for cls in expected_classes)
    
    # äºˆæ¸¬ãƒ†ã‚¹ãƒˆ
    test_text = ["æ”¯æ‰•ã„æœŸé™ã®ãŠçŸ¥ã‚‰ã› æ–™é‡‘ã®å¼•ãè½ã¨ã—"]
    X_test = vectorizer.transform(test_text)
    prediction = model.predict(X_test)
    
    assert len(prediction) == 1
    assert prediction[0] in expected_classes

def test_extended_text_classification():
    """æ‹¡å¼µç‰ˆãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ãƒ†ã‚¹ãƒˆ"""
    df = create_extended_training_data()
    vectorizer, model = train_extended_model()
    
    # æ”¯æ‰•ã„é–¢ä¿‚ã®ãƒ†ã‚¹ãƒˆï¼ˆã‚ãªãŸã®ãƒ¡ãƒ¼ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    payment_texts = [
        "ãƒ‡ãƒ“ãƒƒãƒˆã‚«ãƒ¼ãƒ‰ ã”åˆ©ç”¨ã®ãŠçŸ¥ã‚‰ã› ä½ä¿¡SBI",
        "PayPayæ®‹é«˜ãƒãƒ£ãƒ¼ã‚¸å®Œäº† æ–™é‡‘å¼•ãè½ã¨ã—",
        "Netflix æœˆé¡æ–™é‡‘ æ±ºæ¸ˆå®Œäº†"
    ]
    
    for text in payment_texts:
        X_test = vectorizer.transform([text])
        prediction = model.predict(X_test)[0]
        # æ”¯æ‰•ã„é–¢ä¿‚ã¨ã—ã¦åˆ†é¡ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        assert prediction in ['æ”¯æ‰•ã„é–¢ä¿‚', 'ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³', 'é‡è¦', 'ä»•äº‹ãƒ»å­¦ç¿’']
    
    # ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³é–¢ä¿‚ã®ãƒ†ã‚¹ãƒˆ
    promo_texts = [
        "Amazon ã‚¿ã‚¤ãƒ ã‚»ãƒ¼ãƒ« æœŸé–“é™å®š",
        "æ¥½å¤©å¸‚å ´ ãŠè²·ã„ç‰©ãƒãƒ©ã‚½ãƒ³ ãƒã‚¤ãƒ³ãƒˆæœ€å¤§",
        "Udemy ãƒ“ãƒƒã‚°ã‚»ãƒ¼ãƒ« 90%ã‚ªãƒ•"
    ]
    
    for text in promo_texts:
        X_test = vectorizer.transform([text])
        prediction = model.predict(X_test)[0]
        assert prediction in ['æ”¯æ‰•ã„é–¢ä¿‚', 'ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³', 'é‡è¦', 'ä»•äº‹ãƒ»å­¦ç¿’']
    
    # é‡è¦ãƒ»ã‚·ã‚¹ãƒ†ãƒ é–¢ä¿‚ã®ãƒ†ã‚¹ãƒˆ
    important_texts = [
        "ç·Šæ€¥ ã‚·ã‚¹ãƒ†ãƒ éšœå®³ ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼",
        "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¢ãƒ©ãƒ¼ãƒˆ ä¸æ­£ã‚¢ã‚¯ã‚»ã‚¹",
        "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å¤‰æ›´ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å‘ä¸Š"
    ]
    
    for text in important_texts:
        X_test = vectorizer.transform([text])
        prediction = model.predict(X_test)[0]
        assert prediction in ['æ”¯æ‰•ã„é–¢ä¿‚', 'ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³', 'é‡è¦', 'ä»•äº‹ãƒ»å­¦ç¿’']
    
    # ä»•äº‹ãƒ»å­¦ç¿’é–¢ä¿‚ã®ãƒ†ã‚¹ãƒˆ
    work_texts = [
        "Indeed æ–°ç€æ±‚äºº ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢è·",
        "GitHub ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã‚µãƒãƒªãƒ¼ ã‚³ãƒŸãƒƒãƒˆ",
        "Udemy å­¦ç¿’é€²æ— Pythonå…¥é–€ã‚³ãƒ¼ã‚¹"
    ]
    
    for text in work_texts:
        X_test = vectorizer.transform([text])
        prediction = model.predict(X_test)[0]
        assert prediction in ['æ”¯æ‰•ã„é–¢ä¿‚', 'ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³', 'é‡è¦', 'ä»•äº‹ãƒ»å­¦ç¿’']

def test_specific_service_classification():
    """ç‰¹å®šã‚µãƒ¼ãƒ“ã‚¹ã®åˆ†é¡ãƒ†ã‚¹ãƒˆ"""
    df = create_extended_training_data()
    vectorizer, model = train_extended_model()
    
    # ã‚ãªãŸãŒã‚ˆãä½¿ã†ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ†ã‚¹ãƒˆ
    service_tests = [
        ("OPENAI CHATGPT SUBSCR ãƒ‡ãƒ“ãƒƒãƒˆã‚«ãƒ¼ãƒ‰åˆ©ç”¨", "æ”¯æ‰•ã„é–¢ä¿‚"),
        ("ã‚»ãƒ–ãƒ³ãƒã‚¤ãƒ«ãƒ—ãƒ­ã‚°ãƒ©ãƒ  æ–°ç€ç‰¹å…¸", "ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³"),
        ("povo ãƒ‡ãƒ¼ã‚¿è¿½åŠ  ãƒˆãƒƒãƒ”ãƒ³ã‚°", "ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³"),
        ("Indeed ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ä¼šå“¡ æ±‚äººæƒ…å ±", "ä»•äº‹ãƒ»å­¦ç¿’"),
        ("Udemy ã‚³ãƒ¼ã‚¹ä¿®äº†è¨¼æ˜æ›¸", "ä»•äº‹ãƒ»å­¦ç¿’"),
        ("GitHub ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¢ãƒ©ãƒ¼ãƒˆ", "é‡è¦")
    ]
    
    for text, expected_category in service_tests:
        X_test = vectorizer.transform([text])
        prediction = model.predict(X_test)[0]
        
        # å®Œå…¨ä¸€è‡´ã¯æ±‚ã‚ãªã„ãŒã€4ã‚«ãƒ†ã‚´ãƒªå†…ã§åˆ†é¡ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        assert prediction in ['æ”¯æ‰•ã„é–¢ä¿‚', 'ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³', 'é‡è¦', 'ä»•äº‹ãƒ»å­¦ç¿’']
        print(f"ãƒ†ã‚¹ãƒˆ: {text[:30]}... â†’ äºˆæ¸¬: {prediction} (æœŸå¾…: {expected_category})")

def test_model_persistence():
    """ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ"""
    import pickle
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    vectorizer, model = train_extended_model()
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model_path = 'test_model.pkl'
    with open(model_path, 'wb') as f:
        pickle.dump((vectorizer, model), f)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚ŒãŸã‹ç¢ºèª
    assert os.path.exists(model_path)
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    with open(model_path, 'rb') as f:
        loaded_vectorizer, loaded_model = pickle.load(f)
    
    # èª­ã¿è¾¼ã¾ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒå‹•ä½œã™ã‚‹ã‹ç¢ºèª
    test_text = ["ãƒ†ã‚¹ãƒˆãƒ¡ãƒ¼ãƒ« åˆ†é¡ç¢ºèª"]
    X_test = loaded_vectorizer.transform(test_text)
    prediction = loaded_model.predict(X_test)
    
    assert len(prediction) == 1
    assert prediction[0] in ['æ”¯æ‰•ã„é–¢ä¿‚', 'ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³', 'é‡è¦', 'ä»•äº‹ãƒ»å­¦ç¿’']
    
    # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
    os.remove(model_path)

def test_confidence_scores():
    """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ãƒ†ã‚¹ãƒˆ"""
    df = create_extended_training_data()
    vectorizer, model = train_extended_model()
    
    # æ˜ç¢ºãªåˆ†é¡ã‚±ãƒ¼ã‚¹
    clear_cases = [
        "PayPay æ±ºæ¸ˆå®Œäº† æ”¯æ‰•ã„",
        "Amazon ã‚»ãƒ¼ãƒ« æœŸé–“é™å®š",
        "ç·Šæ€¥ ã‚·ã‚¹ãƒ†ãƒ éšœå®³",
        "Indeed æ±‚äºº ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢"
    ]
    
    for text in clear_cases:
        X_test = vectorizer.transform([text])
        prediction = model.predict(X_test)[0]
        confidence_scores = model.decision_function(X_test)[0]
        max_confidence = max(confidence_scores) if hasattr(confidence_scores, '__iter__') else confidence_scores
        
        # ä¿¡é ¼åº¦ãŒè¨ˆç®—ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
        assert isinstance(max_confidence, (int, float))
        print(f"ãƒ†ã‚­ã‚¹ãƒˆ: {text} â†’ åˆ†é¡: {prediction}, ä¿¡é ¼åº¦: {max_confidence:.2f}")

if __name__ == '__main__':
    print("Gmailåˆ†é¡PoC - æ‹¡å¼µç‰ˆãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    print("=" * 50)
    
    # å€‹åˆ¥ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_extended_sample_data_creation()
    print("âœ… ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆãƒ†ã‚¹ãƒˆ: æˆåŠŸ")
    
    test_extended_model_training()
    print("âœ… ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãƒ†ã‚¹ãƒˆ: æˆåŠŸ")
    
    test_extended_text_classification()
    print("âœ… ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ãƒ†ã‚¹ãƒˆ: æˆåŠŸ")
    
    test_specific_service_classification()
    print("âœ… ç‰¹å®šã‚µãƒ¼ãƒ“ã‚¹åˆ†é¡ãƒ†ã‚¹ãƒˆ: æˆåŠŸ")
    
    test_model_persistence()
    print("âœ… ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ: æˆåŠŸ")
    
    test_confidence_scores()
    print("âœ… ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ãƒ†ã‚¹ãƒˆ: æˆåŠŸ")
    
    print("\nğŸ‰ å…¨ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
    
    # pytestå®Ÿè¡Œ
    # pytest.main([__file__, '-v'])